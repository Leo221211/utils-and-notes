{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import data\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Extract feature and target arrays\n",
    "# X is all the other features and y is the price\n",
    "X, y = diamonds.drop('price', axis=1), diamonds[['price']]\n",
    "\n",
    "# Extract text features\n",
    "# for the features that are not np.number, convert them to type 'category'. (eg. color column is 'blue', 'green' ...)\n",
    "# (for xgboost, category datatype can be self transformed and recognised by the model, no need to manually do one hot)\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "for col in cats:\n",
    "    X[col] = X[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train test split\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# read data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n",
    "# create model instance\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)\n",
    "# make predictions\n",
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost training with native API, using regression as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"convert the data to XGBoost specified DMatrix\"\"\"\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"specify hyper params in dict\"\"\"\n",
    "# Define hyperparameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "\n",
    "n = 100\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=n,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing\n",
    "\n",
    "result is an np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(dtest_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other training objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. classification\n",
    "\n",
    "1. Just need to change the objective funciton\n",
    "   The two most popular classification objectives are:\n",
    "  > binary:logistic - binary classification (the target contains only two classes, i.e., cat or dog)   \n",
    "  > multi:softprob - multi-class classification (more than two classes in the target, i.e., apple/orange/banana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"multi-class (binary class is similar)\"\"\"\n",
    "\n",
    "'''data'''\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]\n",
    "# Encode y to numeric\n",
    "# XGBoost only accepts number in tartget\n",
    "y_encoded = OrdinalEncoder().fit_transform(y)\n",
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "    X[col] = X[col].astype('category')\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)\n",
    "# Create classification matrices\n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "\n",
    "'''train'''\n",
    "params = {\"objective\": \"multi:softprob\", \"tree_method\": \"gpu_hist\", \"num_class\": 5}\n",
    "n = 1000\n",
    "results = xgb.cv(\n",
    "    params, dtrain_clf,\n",
    "    num_boost_round=n,\n",
    "    nfold=5,\n",
    "    metrics=[\"mlogloss\", \"auc\", \"merror\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiclass classification\n",
    "\n",
    "just that Input y should be of shape (n_samples, n_classes) with each column having a value of 0 or 1 to specify whether the sample is labeled as positive for respective class."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
