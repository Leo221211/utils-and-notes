{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "website: [LIGHTNING IN 15 MINUTES](https://lightning.ai/docs/pytorch/stable/starter/introduction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install lightning -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lightning module define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "\n",
    "class Pl_eg_module(pl.LightningModule):\n",
    "    def __init__(self, hyper_param):\n",
    "        super.__init__()\n",
    "\n",
    "        self.hyper_param = hyper_param\n",
    "\n",
    "        metric = lambda x: x\n",
    "\n",
    "        loss_hist = []\n",
    "\n",
    "        model_layers = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''\n",
    "        defines the training loop\n",
    "        '''\n",
    "\n",
    "        # load batch\n",
    "\n",
    "        # output = self(inputs), then the self calls the forward \n",
    "\n",
    "        # cal loss\n",
    "\n",
    "        # return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Option\n",
    "        Executes after each epochs end\n",
    "        return type is None (don't need to write return statement)\n",
    "        '''\n",
    "\n",
    "    '''\n",
    "    Also have validation_step, test_step, on_validation_epoch_end, etc\n",
    "    '''\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class pl_eg_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X[idx], self.y[idx] \n",
    "\n",
    "class pl_eg_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=64, num_workers=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # pre-initialize the datasets\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "        \"\"\"\n",
    "        stage (str): 'fit' or 'test'\n",
    "        \"\"\"\n",
    "        train_fasta_path = Path(self.data_dir, 'fold0_train.fasta')\n",
    "        self.train_dataset = pl_eg_Dataset(train_fasta_path)\n",
    "\n",
    "        '''optional'''\n",
    "        val_fasta_path = Path(self.data_dir, 'fold0_val.fasta')\n",
    "        test_fasta_path = Path(self.data_dir, 'fold0_test.fasta')                 \n",
    "        self.val_dataset = pl_eg_Dataset(val_fasta_path)\n",
    "        self.test_dataset = pl_eg_Dataset(test_fasta_path)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "\n",
    "data_module = pl_eg_DataModule(data_dir='./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what it actually does is:\n",
    "\n",
    "```\n",
    "model = Pl_eg_module(lr=1e-4, weight_decay=1e-4)\n",
    "optimizer = model.configure_optimizers()\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloaders.train_dataloader):\n",
    "    loss = model.training_step(batch, batch_idx)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "1. other params for pl.Trainer():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Pl_eg_module(lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=model_dir, filename='{epoch:02d}-{val_loss:.2f}', every_n_epochs=1, save_top_k=-1)        # save every epoch\n",
    "# checkpoint_callback = ModelCheckpoint(dirpath=model_dir, monitor='val_loss', filename='{epoch:02d}-{val_loss:.2f}', every_n_epochs=1, save_top_k=1)        # save best epoch\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=59, \n",
    "                        default_root_dir = results_dir,        # save the lightning_log\n",
    "                        callbacks=[checkpoint_callback],       # save the checkpoint\n",
    "                        accelerator='auto', devices='auto', strategy='auto')    # this line for gpu acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test the model after train\n",
    "\n",
    "## basic\n",
    "1. Use trainer.test(), agonistic to trainer.fit()\n",
    "1. Run one epoch of test defined under test_step()\n",
    "1. note: due to DDP issues, better set Trainer(devices=1)\n",
    "\n",
    "## params of test()\n",
    "Trainer.test(model=None, dataloaders=None, ckpt_path=None, verbose=True, datamodule=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, dataloaders=DataLoader(test_set))\n",
    "\n",
    "# '''test a model from ckpt path'''\n",
    "# trainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\n",
    "# # can also do ckpt_path=\"best\", ckpt_path=\"last\"; automatically tracked in the trainer\n",
    "\n",
    "# '''test a model by giving it a model'''\n",
    "# model1 = LitModel()\n",
    "# model2= MyLightningModule.load_from_checkpoint(checkpoint_path=\"/path/to/pytorch_checkpoint.ckpt\", \n",
    "#                                                hparams_file=\"/path/to/experiment/version/hparams.yaml\", map_location=None)\n",
    "\n",
    "# trainer.test(model1)\n",
    "# trainer.test(model2)\n",
    "\n",
    "# '''specify dataloaders'''\n",
    "# # The default dataloader is the test dataloader specified in the LightningModule. But can overwrite (or define from scratch) from here\n",
    "\n",
    "# test_dataloader = DataLoader(...)\n",
    "# trainer.test(dataloaders=test_dataloader)\n",
    "\n",
    "'''tested OK'''\n",
    "data_module = xxx (pl.LightningDataModule)\n",
    "model = siteLevelModel.load_from_checkpoint(checkpoint_path=model_path, map_location=None)\n",
    "\n",
    "trainer = pl.Trainer(deterministic=False,\n",
    "                         devices=1,\n",
    "                         logger=wandb_logger,\n",
    "                         default_root_dir = results_dir,\n",
    "                         accelerator='auto', strategy='auto')\n",
    "\n",
    "trainer.test(model=pretrained_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Load and use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./results/epoch=0-step=100.ckpt\"\n",
    "model = Pl_eg_module.load_from_checkpoint(checkpoint)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger\n",
    "\n",
    "## general notes\n",
    "\n",
    "1. save dir: defualt is ```getcwd()```. To specify: ```Trainer(default_root_dir=\"/your/path/to/save/checkpoints\")```\n",
    "1. log writing frequency \\\n",
    "By default, Lightning logs every 50 rows, or 50 training steps. To change: \\\n",
    "```trainer = Trainer(log_every_n_steps=100)```\n",
    "1. Resuming \\\n",
    "Lightning ckpt is compatible with torch.load. \\\n",
    "Resume training state, simply:```trainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")```\n",
    "\n",
    "### self.log()\n",
    "1. 2 basig log method: log() and log_dict(), can be used everywhere in Lightning modules and callbacks \\\n",
    "1. usage: \\\n",
    "log(): ```self.log(\"my_metric\", x)``` \\\n",
    "log_dict(): ```self.log_dict({\"acc\": acc, \"recall\": recall})```\n",
    "\n",
    "1. arguments (apply for both methods:\\\n",
    "```prog_bar```: Logs to the progress bar (Default: False). \\\n",
    "```logger```: Logs to the logger like Tensorboard, or any other custom logger passed to the Trainer (Default: True). \\\n",
    "```on_step```: Logs the metric at the current step. \\\n",
    "```on_epoch```: Automatically accumulates and logs at the end of the epoch.\n",
    "\n",
    "### saving hyper parameters\n",
    "\n",
    "Hyperparams are just the arguments passed to the ```__init()__```\n",
    "Use ```self.save_hyperparameters()``` in ```__init__()``` method. Then the hyperparams will be stored with the checkpoint. \n",
    "\n",
    "1. to only save some: \\\n",
    "self.save_hyperparameters(\"layer_1_dim\")\n",
    "\n",
    "1. to exclude some: \\\n",
    "self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])\n",
    "\n",
    "1. If the hyper param are included in the init then don't care. They're loaded into the model automatically. If need to change / specify, just xxx=xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default logger: ```TensorBoard```\n",
    "\n",
    "1. stores the logs to a directory (by default in lightning_logs/)\n",
    "1. visualize \n",
    "```\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=/data2/mqyu/work/HeLa_cross_tissue_test_wb/1_train_on_HeLa_backup/lightning_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb (weight and bias)\n",
    "link for wandb: click [here](https://docs.wandb.ai/guides/integrations/lightning/)\n",
    "\n",
    "1. create account and loggin. \n",
    "\n",
    "1. before and after the trainer, see below. In the training just the same as using any logger\n",
    "\n",
    "1. Go to the website at https://wandb.ai/leoyu20220822/projects to see project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "'''the other codes'''\n",
    "wandb_logger = WandbLogger(project='9-25-1')\n",
    "\n",
    "'''hyper param (?)'''\n",
    "wandb_logger.experiment.config[\"batch_size\"] = batch_size\n",
    "\n",
    "'''train'''\n",
    "trainer = pl.Trainer(limit_train_batches=750, max_epochs=5, logger=wandb_logger)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n",
    "\n",
    "'''optional if not in notebooks'''\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device related\n",
    "\n",
    "1. to check the device of current model: ```print(self.device)```\n",
    "\n",
    "1. no any .to(device) or .cuda()\n",
    "\n",
    "1. When init tensor in the non ```__init__()``` stage: ```new_tensor = new_tensor.to(an_old_tensor)``` (?) just to a existing tensor and it will know\n",
    "\n",
    "1. If declare a tensor in the ```__init__()``` stage, use register buffer: ```self.register_buffer('pos_weight', torch.ones([2,3])```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnafm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
